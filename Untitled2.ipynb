{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    \"\"\"RNNLM模型配置项\"\"\"\n",
    "    embedding_dim = 200  # 词向量维度\n",
    "\n",
    "    rnn_type = 'LSTM'  # 支持RNN/LSTM/GRU\n",
    "    hidden_dim = 200  # 隐藏层维度\n",
    "    num_layers = 1  # RNN 层数\n",
    "\n",
    "    dropout = 0.5  # 丢弃概率\n",
    "    tie_weights = True  # 是否绑定参数\n",
    "\n",
    "    clip = 0.25  # 用于梯度规范化\n",
    "    learning_rate = 0.5  # 初始学习率\n",
    "\n",
    "    log_interval = 500  # 每隔多少个批次输出一次状态\n",
    "    save_interval = 3  # 每个多少个轮次保存一次参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNLM(nn.Module):\n",
    "    \"\"\"基于RNN的语言模型，包含一个encoder，一个rnn模块，一个decoder。\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(RNNLM, self).__init__()\n",
    "\n",
    "        v_size = config.vocab_size\n",
    "        em_dim = config.embedding_dim\n",
    "        dropout = config.dropout\n",
    "        \n",
    "        self.rnn_type = rnn_type = config.rnn_type\n",
    "        self.hi_dim = hi_dim = config.hidden_dim\n",
    "        self.n_layers = n_layers = config.num_layers\n",
    "\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(v_size, em_dim)\n",
    "\n",
    "        # rnn: RNN / LSTM / GRU\n",
    "        self.rnn = getattr(nn, rnn_type)(em_dim, hi_dim, n_layers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(hi_dim, v_size)\n",
    "\n",
    "        # tie_weights将encoder和decoder的参数绑定为同一参数。\n",
    "        if config.tie_weights:\n",
    "            if hi_dim != em_dim:  # 这两个维度必须相同\n",
    "                raise ValueError('When using the tied flag, hi_dim must be equal to em_dim')\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "\n",
    "        self.init_weights()  # 初始化权重\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        seq_len = len(inputs)\n",
    "        emb = self.drop(self.encoder(inputs).view(seq_len, 1, -1))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.decoder(output.view(seq_len, -1))\n",
    "        return output, hidden  # 复原\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"权重初始化，如果tie_weights，则encoder和decoder权重是相同的\"\"\"\n",
    "        init_range = 0.1\n",
    "        self.encoder.weight.data.uniform_(-init_range, init_range)\n",
    "        self.decoder.weight.data.uniform_(-init_range, init_range)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        \"\"\"初始化隐藏层\"\"\"\n",
    "        weight = next(self.parameters()).data\n",
    "        if self.rnn_type == 'LSTM':  # lstm：(h0, c0)\n",
    "            return (Variable(weight.new(self.n_layers, 1, self.hi_dim).zero_()),\n",
    "                    Variable(weight.new(self.n_layers, 1, self.hi_dim).zero_()))\n",
    "        else:  # gru 和 rnn：h0\n",
    "            return Variable(weight.new(self.n_layers, 1, self.hi_dim).zero_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def open_file(filename, mode='r'):\n",
    "    return open(filename, mode=mode, encoding='utf-8', errors='ignore')\n",
    "\n",
    "class Corpus(object):\n",
    "    \"\"\"\n",
    "    文本预处理，获取词汇表，并将字符串文本转换为数字序列。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train_dir, vocab_dir):\n",
    "        assert os.path.exists(train_dir), 'File %s does not exist.' % train_dir\n",
    "        \n",
    "        if not os.path.exists(vocab_dir):\n",
    "            words = list(set(list(open_file(train_dir).read().replace('\\n', ''))))\n",
    "            open_file(vocab_dir, 'w').write('\\n'.join(sorted(words)) + '\\n')\n",
    "        \n",
    "        words = open_file(vocab_dir).read().strip().split('\\n')\n",
    "        word_to_id = dict(zip(words, range(len(words))))\n",
    "        \n",
    "        data = []\n",
    "        with open_file(train_dir) as f:\n",
    "            for line in f:\n",
    "                poem = [word_to_id[x] for x in line.strip() if x in word_to_id]\n",
    "                data.append(poem)\n",
    "        \n",
    "        self.words = words\n",
    "        self.word_to_id = word_to_id\n",
    "        self.data = data\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Corpus length: %d, Vocabulary size: %d\" % (len(self.data), len(self.words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Corpus length: 51836, Vocabulary size: 7353"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config()\n",
    "corpus = Corpus('data/poem.tang.txt', 'data/poem.vocab.txt')\n",
    "config.vocab_size = len(corpus.words)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = RNNLM(config)\n",
    "if use_cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(model, words, word_len=50, temperature=1.0):\n",
    "    \"\"\"生成一定数量的文本，temperature结合多项式分布可增添抽样的多样性。\"\"\"\n",
    "    model.eval()\n",
    "    hidden = model.init_hidden()  # batch_size为1\n",
    "    inputs = Variable(torch.rand(1, 1).mul(len(words)).long(), volatile=True)  # 随机选取一个字作为开始\n",
    "    if use_cuda:\n",
    "        inputs = inputs.cuda()\n",
    "\n",
    "    word_list = []\n",
    "    for i in range(word_len):  # 逐字生成\n",
    "        output, hidden = model(inputs, hidden)\n",
    "        word_weights = output.squeeze().data.div(temperature).exp().cpu()\n",
    "\n",
    "        # 基于词的权重，对其再进行一次抽样，增添其多样性，如果不使用此法，会导致常用字的无限循环\n",
    "        word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "        inputs.data.fill_(word_idx)  # 将新生成的字赋给inputs\n",
    "        word = words[word_idx]\n",
    "        word_list.append(word)\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate2(model, words, word_len=50, temperature=1.0):\n",
    "    \"\"\"生成一定数量的文本，temperature结合多项式分布可增添抽样的多样性。\"\"\"\n",
    "    model.eval()\n",
    "    hidden = model.init_hidden()  # batch_size为1\n",
    "    inputs = Variable(torch.rand(1, 1).mul(len(words)).long(), volatile=True)  # 随机选取一个字作为开始\n",
    "    if use_cuda:\n",
    "        inputs = inputs.cuda()\n",
    "\n",
    "    word_list = []\n",
    "    for i in range(word_len):  # 逐字生成\n",
    "        output, hidden = model(inputs, hidden)\n",
    "        topv, topi = output.data.topk(1)\n",
    "        word_idx = topi[0][0]\n",
    "        inputs.data.fill_(word_idx)  # 将新生成的字赋给inputs\n",
    "        word = words[word_idx]\n",
    "        word_list.append(word)\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.1420595703125\n",
      "6.15007958984375\n",
      "缑，持日回流浮无一夜。分境有之父，同门迟绳。须是日落，岂遇哢望冥寒。儒纳失长乡，怡军闭鸟游。傍泠尚接\n",
      "6.16421923828125\n",
      "6.21496435546875\n",
      "邑月深，到闻杏寥轮行舞芭秋风。远沉卷小下曲，劝央听烟。到雨云扇军酸经，晨第农人。不共皆笑风尽倦春去，\n",
      "6.12598095703125\n",
      "6.15830908203125\n",
      "泥辞，临看拂千里在。洞得有虎知，和波藏天禅祚。簷戒宣木，美期反经陈。笙花丛读一醉后，碧云木双东。田受\n",
      "6.16870068359375\n",
      "6.19273095703125\n",
      "生映。肯更自念百余，居主柳境。乳峰犹江水榆雁兔眼，烟生初思口闲。草君断竹，依轮始恐臣。文渠响今，重若\n",
      "6.14020263671875\n",
      "6.2198408203125\n",
      "渥得，灵社此地。歎猨鬓晓，不可关不离。使少营会，夜为晓行天陌生。莲草前渡忽本，长得乡妃。为内满去杖，\n",
      "6.13662255859375\n",
      "6.18655029296875\n",
      "秋破檝，河历烛药。功惨在殿称，且望行故人美。梅晏此晚，九清风烟。回水汀外，手自访。边坛相引后，两秋色\n",
      "6.1824189453125\n",
      "6.17602490234375\n",
      "一挥，山天籍树。桐生还生良，天花间语冰萍尔。逢是时传，应嫌国中酒。蓬歌无黄叶，年使深何处来爱。何况堪\n",
      "6.16453173828125\n",
      "6.13913720703125\n",
      "当迹，夜窗沧波满雅。幽载鹤月不相去，浮峰自聚不恋城。留伯鲈齐饥何处死，白寂寞被冲。鼎觞澄柳妙子，用图\n",
      "6.17518310546875\n",
      "6.17148681640625\n",
      "，未要花。人莫眠云，一船倚火元渐同。不流掩桂寻耀数，分家日河无殊。百日月结郡，相逢寄趋如十里酒。后移\n",
      "6.18845703125\n",
      "6.21372021484375\n",
      "过暖刀清夜君。张巢换来，携树耳开。唯相如此，林面分羽珊病下台蚕。当烟期酌，莫自垂塘。王岭役者间玉，山\n",
      "6.15478857421875\n",
      "6.15011865234375\n",
      "携待下读莱未哉哉残知，不似输三月残。三峡逼，笑明朝此南。休耻入方，掇岂迟。为宗暑山何昼数标形，日对不\n",
      "6.158185546875\n",
      "6.17083935546875\n",
      "，焉弦日无阳独望。六夕是辨，晓猿多依近忙。积伐陪妆足，来虚乡杏雪多新夫外长。更游表深和九落争，绿甲歌\n",
      "6.19940087890625\n",
      "6.16413671875\n",
      "篸，乳胡国。山稽似事穿上，嬴鱼乎。年道日望恨，自为炉。北是庭如练去，虚先德影先丝。方豫秉不埽，废迷定\n",
      "6.15786865234375\n",
      "6.2209482421875\n",
      "食野塘翠，金潮长。常易思山，此为里万年宸。留上飞社小，花随归山阴。不共里日，朝蛛霜歇。爱巢此日，不如\n",
      "6.15953662109375\n",
      "6.18019091796875\n",
      "皇青子，掩我有处。玉搔瑶野舍，长路长恨黛尘。到采季云坞桥，人忆书双歌书。簷朗师紫，死鲍汉人莫圆。风入\n",
      "6.20799658203125\n",
      "6.21403955078125\n",
      "月俗壁益事时，风夜穷机谒。行拂古如，烦照如家星兔陆。不携出亦过，贪临门浦。不多蔡贾偏，山如僧，断劒今\n",
      "6.16365478515625\n",
      "6.19181640625\n",
      "石人去，览塞雪恐花齐叶香天宫。未取终茫不是魔，旋帏琼携同付。从君万路花过，任院多辽望沈。应见楼枕静隔\n",
      "6.17377587890625\n",
      "6.2083173828125\n",
      "人心偏过，日渐辞营浩臣。当丝道千里栖，逍似书长闻恒。未见头月，江上心老阴凄轮须须过。春闲不倦，长白吟\n",
      "6.2119267578125\n",
      "6.1766015625\n",
      "菊扫苑含场竹，经阙言清一室华。新不忧仙水气隐，不知忆地修命。桥飞雨日边松，雾孙难来见舟更归。红叶愿空\n",
      "6.1907373046875\n",
      "6.2025458984375\n",
      "是先徐残雨琪淮倦天，须远渭堂人。君礼人觅，天地远。长年文不事才，渐言直淡足。皇牵云滴，无好夕不。言博\n",
      "6.2074765625\n",
      "6.1658212890625\n",
      "消，将管标犹。自低雾飞寒，鸡瓶蔡，焉本他居今北无旧把送昭。何年国入去，赖疲今须来。见争窓飞聊常，高淫\n",
      "6.1353701171875\n",
      "6.19508154296875\n",
      "质场桥日人皆营裁悲姓望气澜，只复重足万星饵脉。异声先事如门尽王子，无隔膏亲。虫馆钟百尺乘，逍桐不然水\n",
      "6.2186083984375\n",
      "6.16402001953125\n",
      "里莫然吾奏云随吐，寂寂寂寞微，荣唤高时。新接万子，指童一通属台。石樯体亡，故人尚出搜红古。绿昌忧透青\n",
      "6.19394580078125\n",
      "6.17597412109375\n",
      "我静，执汉暗无子。秦觅河方小，上笛帘雨闻通节。同年磨琴，接旅杨烟满。已悲寒色，鸣刺洲家心。所凄后，闻\n",
      "6.2384326171875\n",
      "6.1629482421875\n",
      "密山景数谢色心，解冲吴鹿丝乡翠闲越，无霜结难违寄柳院簇。无因鱼京一种，怜石变。五向拟占白，刀屐前柏疎\n",
      "6.18045068359375\n",
      "6.17512646484375\n",
      "是峰探仍寒君成开添能息尔玄步，君睨顾灵。皓江已缘宿，野谁道笙拥薜。如今莫承岂从，几安销红亦拜游坛。却\n",
      "6.18042626953125\n",
      "6.2111025390625\n",
      "马下迎洞，夜田。今夜欹崙且能别，无形愁少都认。莫把有酒弓芍岭嶂，开生凤杨花。阴垂有何许重，黄今不似岭\n",
      "6.20723388671875\n",
      "6.13244189453125\n",
      "北声心兮天，枫无无谋。六亭不易适，飞人同冷张唤。况登头感华月，即陪赠松鸟校寻。为根禅泉样，密酒当火心\n",
      "6.22533935546875\n",
      "6.2037490234375\n",
      "留戍山，林化暑秉。龟阴锦悟，不知未怜秋兴凉。云色华路谁摩，君能向奇池青。只解兰无无有客，衰笳月。降三\n",
      "6.1779365234375\n",
      "6.21485888671875\n",
      "明谁生，更深装恐，养以光认蒙漫云。引奉钟精，受到内弦。老就携竹更少，向湘望行相见秋草。江南有气，乃既\n",
      "6.15617236328125\n",
      "6.21060546875\n",
      "苦南北晖须曛，岂是内本户垂。独恋泪澄，艰水泪争汗荣。路度通人一物，陇向白花日。公叶能师，周声须旋蝴根\n",
      "6.23745068359375\n",
      "6.1606875\n",
      "繁楼乌极认。每色今人自朝，声斋学豔远。穷下漫里闲，望匪须去三身。断定鬓士瓦，文泪霜居。宿嵩宫三代，明\n",
      "6.220154296875\n",
      "6.1959541015625\n",
      "将云水霜发翠孤平可作，四句经后苔子。只把自疑离坐，辉邸炉草义水峒。巧栾愁雨，尘静心家。歌春依暮，空虑\n",
      "6.24831787109375\n",
      "6.2263935546875\n",
      "如路才客夏定耳高，渡到石隔面宗。吴议女归，萧曹燕石松臯。月圆风至落，寄乌傍归空。干知池行居，出禽谢皆\n",
      "6.16853564453125\n",
      "6.172162109375\n",
      "伍龙呼苦荆身思慵暮，密散筑病生亲少。经园山缘好发，丹道名。孤舟泪弦箓，莫使几成牵山身。空土玉何消，妍\n",
      "6.22712109375\n",
      "6.19769140625\n",
      "氲，垂缀非诗。良是恩栖向轻灵，相向升随生寒吹。不有色犹未道，寒贮上塘。轻香日归时对雨，纵变病立半，乱\n",
      "6.17861328125\n",
      "6.16605859375\n",
      "矶。倚我严蘂皆，吟上舞翠地醒。去来乐驾，由凌朱栋。白来书莹雨，六水自我。心耿纨，文卷心草。坐声非一色\n",
      "6.21006787109375\n",
      "6.20741796875\n",
      "问纸隐。满臣义青壁罢塞，竹叶深欢鸣。坐销在卓暗到，迎桥断几家。吴虹将琅当伤雁，三年不复来。入潜空外里\n",
      "6.25029052734375\n",
      "6.2292255859375\n",
      "积，世乱谁霜禹出儒。礼到车，幸鞭想辰劝。碧情竹称变鸾，幽家高数莫行是前语杳鼇白正尽。同分咏非中远只，\n",
      "6.20739697265625\n",
      "6.1773828125\n",
      "别，汤门难往衣山春命草。泊鲙明光，寂唯有伴吾遣齐思封。暂觉忽隔，牧往水。不将题神，江流不死鱼。更比不\n",
      "6.19270947265625\n",
      "6.20578466796875\n",
      "果余多飞无极郑青变翔死郡，纵照出紫隐衣。逍民还后征后前迷，雪壁苍后九处苦。千枝那重凄，细山下桂。雪奸\n",
      "6.18703125\n",
      "6.22297265625\n",
      "气向暂远，蒹向娇蔡闻。好丝苦春，行里唱身毕如。成见口脸别，无因莫入小须道。芰广戏弃身，未忍间久。先共\n",
      "6.2041923828125\n",
      "6.19713427734375\n",
      "正厌旒新，一日从蒲。垂激钧，唯看底下。天凰采泥重，恰异将新落泥。寒关又久，行来叶因鬓。晴背梦住，中山\n",
      "6.2242529296875\n",
      "6.243060546875\n",
      "意休小，不惜西所。以秀马鬓，更厌云。醉垂近分尘天，玉髴怀空。谁歌火僧，服浪德歌苦中。行诗北当欲以，见\n",
      "6.2287822265625\n",
      "6.185099609375\n",
      "飞辘念客公涪宿防玄玉醉，利巷有极。蝉共翫才阴铜禁，怜磬那能回。退侵下支，明月下雪冬。此事云暮两兴，汉\n",
      "6.2014853515625\n",
      "6.240640625\n",
      "窥乱卷云横仙，那尝当花霄亭流草。灵泣刘子，人重幷拾。珠王黄声自感，征鸯无句。异成台诗何处谿剑觅外宫溪\n",
      "6.2205654296875\n",
      "6.18378125\n",
      "道集中色疆，色编毒顾及。不得将起黄，悄恨河清。鸣与城花子，为目顇成。公雪尚解，君念得恨翁扉。玉漫多青\n",
      "6.24913916015625\n",
      "6.19093896484375\n",
      "亭溜，争露前香籞起头。异榴高泛壮可此，敢不羞清。秦缨蹋处，不向国欲令深。惆怅往垆，吏髪峰花。向侍溪去\n",
      "6.17622021484375\n",
      "6.213953125\n",
      "斗郎堤，郊堦溜尊真。遍草枕臣疎，朝妾有华。白屋长江湖风促少何处三清吟犹兼，白愁向下。今来乌荐时新，偶\n",
      "6.21876416015625\n"
     ]
    }
   ],
   "source": [
    "total_loss = 0.0\n",
    "for i in range(50000):\n",
    "    model.train()\n",
    "    rand_data = random.choice(corpus.data)\n",
    "    inputs = Variable(torch.LongTensor(rand_data[:-1]))\n",
    "    targets = Variable(torch.LongTensor(rand_data[1:]))\n",
    "    \n",
    "    if use_cuda:\n",
    "        inputs = inputs.cuda()\n",
    "        targets = targets.cuda()\n",
    "\n",
    "    hidden = model.init_hidden()\n",
    "    outputs, hidden = model(inputs, hidden)\n",
    "    loss = criterion(outputs, targets)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    total_loss += loss\n",
    "    if i % config.log_interval == 0 and i > 0:\n",
    "        print(total_loss.data[0] / config.log_interval)\n",
    "        total_loss = 0.0\n",
    "    \n",
    "    if i % 1000 == 0 and i > 0:\n",
    "        gen_words = generate(model, corpus.words)\n",
    "        print(''.join(gen_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
